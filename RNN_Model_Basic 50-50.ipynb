{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bduh-46OHFK",
   "metadata": {
    "id": "3bduh-46OHFK"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b1866-5ebc-42cb-b8f3-13e50355d481",
   "metadata": {},
   "source": [
    "First, we must read in all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iUFDKHaVOHFN",
   "metadata": {
    "id": "iUFDKHaVOHFN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\majon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0182f-e319-4777-a048-ee20ff91846b",
   "metadata": {},
   "source": [
    "In addition to that we need some NLTK datasets with english stopword, that must be removed and lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a7642b-c79a-4752-8864-7d2ffa7edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have downloaded the necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88343b-9426-4b38-b585-8a301e490bd5",
   "metadata": {},
   "source": [
    "Reading the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beLBhgRLOHFP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beLBhgRLOHFP",
    "outputId": "fe017141-8c90-4f18-d260-b91583d3e971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               8000 non-null   int64  \n",
      " 1   _golden                8000 non-null   bool   \n",
      " 2   _unit_state            8000 non-null   object \n",
      " 3   _trusted_judgments     8000 non-null   int64  \n",
      " 4   _last_judgment_at      8000 non-null   object \n",
      " 5   positivity             1420 non-null   float64\n",
      " 6   positivity:confidence  3775 non-null   float64\n",
      " 7   relevance              8000 non-null   object \n",
      " 8   relevance:confidence   8000 non-null   float64\n",
      " 9   articleid              8000 non-null   object \n",
      " 10  date                   8000 non-null   object \n",
      " 11  headline               8000 non-null   object \n",
      " 12  positivity_gold        0 non-null      float64\n",
      " 13  relevance_gold         0 non-null      float64\n",
      " 14  text                   8000 non-null   object \n",
      "dtypes: bool(1), float64(5), int64(2), object(7)\n",
      "memory usage: 882.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./US-Economic-News.csv\", delimiter=',', encoding= 'ISO-8859-1')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2pZ91v4YOHFQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "2pZ91v4YOHFQ",
    "outputId": "2ecb5bf5-5337-4d44-bf14-f45d0725524e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842613455</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>8/14/91</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842613456</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 16:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>8/21/07</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842613457</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 1:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_398284048</td>\n",
       "      <td>11/14/91</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>842613458</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 2:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.675</td>\n",
       "      <td>wsj_397959018</td>\n",
       "      <td>6/16/86</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842613459</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398838054</td>\n",
       "      <td>10/4/02</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  842613455    False   finalized                   3     12/5/15 17:48   \n",
       "1  842613456    False   finalized                   3     12/5/15 16:54   \n",
       "2  842613457    False   finalized                   3      12/5/15 1:59   \n",
       "3  842613458    False   finalized                   3      12/5/15 2:19   \n",
       "4  842613459    False   finalized                   3     12/5/15 17:48   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         3.0                 0.6400       yes                 0.640   \n",
       "1         NaN                    NaN        no                 1.000   \n",
       "2         NaN                    NaN        no                 1.000   \n",
       "3         NaN                 0.0000        no                 0.675   \n",
       "4         3.0                 0.3257       yes                 0.640   \n",
       "\n",
       "       articleid      date                                           headline  \\\n",
       "0  wsj_398217788   8/14/91              Yields on CDs Fell in the Latest Week   \n",
       "1  wsj_399019502   8/21/07  The Morning Brief: White House Seeks to Limit ...   \n",
       "2  wsj_398284048  11/14/91  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3  wsj_397959018   6/16/86  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4  wsj_398838054   10/4/02  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "   positivity_gold  relevance_gold  \\\n",
       "0              NaN             NaN   \n",
       "1              NaN             NaN   \n",
       "2              NaN             NaN   \n",
       "3              NaN             NaN   \n",
       "4              NaN             NaN   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e6c10-3f9a-4d6d-b1ec-da281bc6109f",
   "metadata": {},
   "source": [
    "Removing unneccessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfFHenMOHFQ",
   "metadata": {
    "id": "6dfFHenMOHFQ"
   },
   "outputs": [],
   "source": [
    "df = df[['headline', 'text', 'relevance']]\n",
    "\n",
    "# We drop all irrelavant features to only keep headline and text for 2 reasons: \n",
    "# The other features seem either irrelevant or we lack documentation\n",
    "# With headline and text only, our final model will be more generalizable. We could in theory apply it to any article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b8c92-dfb1-451f-a005-d8cdb366820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_yes = df[df['relevance'] == 'yes']\n",
    "df_no = df[df['relevance'] == 'no']\n",
    "\n",
    "df_no_sampled = df_no.sample(n=len(df_yes), random_state=42)\n",
    "\n",
    "# Concatenate the sampled 'no' rows with all 'yes' rows\n",
    "df_balanced = pd.concat([df_yes, df_no_sampled])\n",
    "\n",
    "print(df_balanced['relevance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd88fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jK-dXnQAOHFR",
   "metadata": {
    "id": "jK-dXnQAOHFR"
   },
   "source": [
    "Cleaning Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hwsu0fkMOHFS",
   "metadata": {
    "id": "hwsu0fkMOHFS"
   },
   "outputs": [],
   "source": [
    "df['whole_txt'] = df['headline']+ ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGK5t8RyOHFT",
   "metadata": {
    "id": "tGK5t8RyOHFT"
   },
   "outputs": [],
   "source": [
    "wtxt_train = np.array(df['whole_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8EX1KU3FOHFT",
   "metadata": {
    "id": "8EX1KU3FOHFT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(wtxt_train)):\n",
    "    # Taking out '<br>' in the 'whole_text' column\n",
    "    wtxt_train[i] = re.sub(r'</?br>', ' ', wtxt_train[i])\n",
    "    # Deletion of non-latin alfabet signs, also numbers\n",
    "    wtxt_train[i] = re.sub(r'[^a-zA-Z]', ' ', wtxt_train[i])\n",
    "    # Removing single letter works like 'a'.\n",
    "    wtxt_train[i] = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', wtxt_train[i])\n",
    "    # Removing double spaces\n",
    "    wtxt_train[i] = re.sub(r'\\s+', ' ', wtxt_train[i])\n",
    "    # Lower case\n",
    "    wtxt_train[i] = wtxt_train[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OGok8jBiOHFT",
   "metadata": {
    "id": "OGok8jBiOHFT"
   },
   "source": [
    "Split the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YLC_hbBPOHFT",
   "metadata": {
    "id": "YLC_hbBPOHFT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = word_tokenize(wtxt_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nJcjnqLHOHFT",
   "metadata": {
    "id": "nJcjnqLHOHFT"
   },
   "source": [
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7uoaH6INOHFT",
   "metadata": {
    "id": "7uoaH6INOHFT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = [word for word in wtxt_train[i] if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74Ii4vxnOHFU",
   "metadata": {
    "id": "74Ii4vxnOHFU"
   },
   "outputs": [],
   "source": [
    "wtxt_train[0]\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Ih9nJCDOHFU",
   "metadata": {
    "id": "0Ih9nJCDOHFU"
   },
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZq0soQFOHFU",
   "metadata": {
    "id": "tZq0soQFOHFU"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = [lemmatizer.lemmatize(word) for word in wtxt_train[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yh7EcIK7OHFU",
   "metadata": {
    "id": "yh7EcIK7OHFU"
   },
   "outputs": [],
   "source": [
    "df['whole_txt'] = wtxt_train\n",
    "df = df.drop(['headline', 'text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T1TLLZZhOHFU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T1TLLZZhOHFU",
    "outputId": "127d4d92-3e49-4449-8bff-a5ab1fa375cc"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ecc91",
   "metadata": {
    "id": "2a9ecc91"
   },
   "source": [
    "### Data preparation\n",
    "* Initial Data Processing: Our first step is to encode the relevance label into both the Relevant (1) and non-Relevant labels (0). Then, we make it into a np.array to feed into the model.\n",
    "* Then, we begin to clean text data into pad sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d046d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(df[\"relevance\"].apply(lambda x: 0 if x == \"no\" else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fe00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc686094",
   "metadata": {
    "id": "bc686094"
   },
   "source": [
    "### Tokenization\n",
    "First, we need to \"tokenize\" our sentences, i.e., convert them to sequences of numbers. For this task, we are going to use the `Tokenizer` from Tensorflow (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eaddf",
   "metadata": {
    "id": "cf9eaddf"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wtxt_train)   # fit our tokenizer on the dataset (i.e., assign a number to each word and keep a\n",
    "                                    # dictionary with the correspondence of each word to a number)\n",
    "\n",
    "# see the language dictionary and the total number of words (please note that number 0 is reserved for the padding task)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb124ce",
   "metadata": {
    "id": "1bb124ce"
   },
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc0b15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8bc0b15",
    "outputId": "bc0c8ab2-9e8f-470d-a17e-89b96cd00afd"
   },
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c30d7",
   "metadata": {
    "id": "016c30d7"
   },
   "source": [
    "### Padding Sequences\n",
    "Sentences and sequences tend to have different lengths, however our model is expecting equally sized observations.\n",
    "Here we want to convert our texts to sequences and make them of the same length (in general, the lenght of the longest of our sequences). We are going to use here `pad_sequences` from Tensorflow (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences)), to add zeroes to the tokenized sentences until they all reach the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea518a6d",
   "metadata": {
    "id": "ea518a6d"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(wtxt_train)\n",
    "padded_sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a19b1",
   "metadata": {
    "id": "6a3a19b1"
   },
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a3374",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf5a3374",
    "outputId": "99253544-bbca-48ca-e1d5-b86689138957"
   },
   "outputs": [],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pad_seq'] = padded_sequences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f444fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['whole_txt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83826301",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences\n",
    "y = df['relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fB_vM2GQkf-Y",
   "metadata": {
    "id": "fB_vM2GQkf-Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0850ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8059d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c078ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8e8b2",
   "metadata": {
    "id": "0fe8e8b2"
   },
   "source": [
    "### Building the model\n",
    "\n",
    "We are going to build a simple model that includes:\n",
    "- `Embedding` layer with an output representation of each word as a vector of dim 16\n",
    "- `LSTM` (see class slides for more detail or RNNs example notebook for more details) with an intermediate state of 100\n",
    "- An output layer `Dense` that connects the output of the LSTM and creates an output of 3 positions (one per class) as output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d316c-3966-4939-b735-79dd2d1ff936",
   "metadata": {},
   "source": [
    "Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02feafd7-481d-4930-8680-4a873789e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101ccfc",
   "metadata": {
    "id": "1101ccfc"
   },
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55429b96",
   "metadata": {},
   "source": [
    "### MODEL 1 (The base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54e838-0fc1-4c04-bc93-08c58aa4804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Change activation based on the number of classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0965d6-b675-4106-82bc-e28b1f13e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f15a2-949e-4342-9550-6454ea7429a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826df8ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "826df8ad",
    "outputId": "954999b9-e6f3-43a2-dc81-a5c10a8e6ef1"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=10, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d9981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1484a19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1484a19",
    "outputId": "5db7ef87-e57e-4f8c-c74b-f124fab03672"
   },
   "outputs": [],
   "source": [
    "print(model(padded_sequences).numpy().argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781be31e",
   "metadata": {},
   "source": [
    "Model 1 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3526a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prection and Confusion Matrix\n",
    "y_pred = model.predict(X_test)\n",
    "bin_y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_y_pred = np.squeeze(bin_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = bin_y_pred\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(f\"{'':<20}{'Predicted No':<20}{'Predicted Yes':<20}\")\n",
    "print(f\"{'Actual No':<20}{TN:<20}{FP:<20}\")\n",
    "print(f\"{'Actual Yes':<20}{FN:<20}{TP:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5437a9f",
   "metadata": {},
   "source": [
    "### MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(total_words,      # number of words to process as input\n",
    "                    50,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model2.add(LSTM(50, return_sequences=False))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fded32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc28f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train, epochs=5, validation_data = (X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8e04a",
   "metadata": {},
   "source": [
    "### MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce897ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model3.add(LSTM(100, return_sequences=False))\n",
    "\n",
    "model3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model3.compile(optimizer='adam', loss='hinge', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "his = model3.fit(X_train, y_train, epochs=5, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67912764",
   "metadata": {},
   "outputs": [],
   "source": [
    "his.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c7755",
   "metadata": {},
   "source": [
    "### MODEL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model4.add(LSTM(100, return_sequences=False))\n",
    "\n",
    "model4.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model4.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba69cf",
   "metadata": {
    "id": "7fba69cf"
   },
   "outputs": [],
   "source": [
    "his2 = model4.fit(X_train, y_train, epochs=5, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf52a58-f735-4808-887c-09cad201429e",
   "metadata": {},
   "source": [
    "### MODEL 5 (On Steriods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f2233-97c0-4479-b19a-5ce5fe5f3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "adamax_opt = Adamax(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20646d43-0589-42c9-a753-c66d1252d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    mask_zero = True,\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model5.add(Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)))\n",
    "\n",
    "model5.add(Dropout(0.2)) \n",
    "\n",
    "model5.add(Bidirectional(tf.keras.layers.LSTM(100, return_sequences=False)))\n",
    "#model5.add(LSTM(100, return_sequences=False))\n",
    "\n",
    "model5.add(Dropout(0.2)) \n",
    "\n",
    "model5.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model5.compile(optimizer=adamax_opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff04abe-608a-4139-8dc7-c2605feac14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57617c-6963-4d24-8134-b48f7a849880",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist5 = model5.fit(X_train, y_train, epochs=10, validation_data = (X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baeb4c7-428f-4c70-8130-cd73eb71441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist5.history['loss'])\n",
    "plt.plot(hist5.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d004f3b-e42c-478f-a440-c4500e03ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model5.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805c04a-2abc-4b8f-b73c-af4bbb1d3f72",
   "metadata": {},
   "source": [
    "### MODEL 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5753f6-c52e-4de6-98b6-9e3228169e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adamax_opt = Adamax(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ebd27-9ff3-4a27-b5fd-96a33f79208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model7.add(LSTM(100, return_sequences=False))\n",
    "#model7.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "\n",
    "model7.add(Dropout(0.2))\n",
    "\n",
    "model7.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model7.compile(optimizer= adamax_opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a474cc-4b69-4df7-836f-8231259a266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist7 = model7.fit(X_train, y_train, epochs=50, validation_data = (X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92acd72-bd15-4ab2-9b45-fd26d1e69f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist7.history['loss'])\n",
    "plt.plot(hist7.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9325fa-0d81-438e-beb8-6cf2df3bfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model7.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd38bd3-1816-4bfd-9971-c2084acbfb78",
   "metadata": {},
   "source": [
    "### MODEL 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4ad03-e817-4836-abd3-4c8e99982d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adamax_opt = Adamax(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fffec0-09d2-455c-8e65-2252a39bf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model9 = Sequential()\n",
    "\n",
    "model9.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "#model9.add(LSTM(100, return_sequences=False))\n",
    "model9.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "\n",
    "model9.add(Dropout(0.2))\n",
    "\n",
    "model9.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model9.compile(optimizer= adamax_opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46365d-29e9-4e02-b0de-b9d190c82c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist9 = model9.fit(X_train, y_train, epochs=50, validation_data = (X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3162d4b-d971-4f23-beb9-ebcb1ba2b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist9.history['loss'])\n",
    "plt.plot(hist9.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bbb10-745f-45b6-862e-a6bbe850443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model9.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f24238-ee65-4615-b759-cc530c16f5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
