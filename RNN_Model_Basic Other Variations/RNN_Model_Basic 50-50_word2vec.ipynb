{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bduh-46OHFK",
   "metadata": {
    "id": "3bduh-46OHFK"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iUFDKHaVOHFN",
   "metadata": {
    "id": "iUFDKHaVOHFN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beLBhgRLOHFP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beLBhgRLOHFP",
    "outputId": "fe017141-8c90-4f18-d260-b91583d3e971"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./US-Economic-News.csv\", delimiter=',', encoding= 'ISO-8859-1')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2pZ91v4YOHFQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "2pZ91v4YOHFQ",
    "outputId": "2ecb5bf5-5337-4d44-bf14-f45d0725524e"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfFHenMOHFQ",
   "metadata": {
    "id": "6dfFHenMOHFQ"
   },
   "outputs": [],
   "source": [
    "df = df[['headline', 'text', 'relevance']]\n",
    "\n",
    "# We drop all irrelavant features to only keep headline and text for 2 reasons: \n",
    "# The other features seem either irrelevant or we lack documentation\n",
    "# With headline and text only, our final model will be more generalizable. We could in theory apply it to any article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_yes = df[df['relevance'] == 'yes']\n",
    "df_no = df[df['relevance'] == 'no']\n",
    "\n",
    "df_no_sampled = df_no.sample(n=len(df_yes), random_state=42)\n",
    "\n",
    "# Concatenate the sampled 'no' rows with all 'yes' rows\n",
    "df_balanced = pd.concat([df_yes, df_no_sampled])\n",
    "\n",
    "print(df_balanced['relevance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd88fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jK-dXnQAOHFR",
   "metadata": {
    "id": "jK-dXnQAOHFR"
   },
   "source": [
    "Cleaning Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kS0pN2EEOHFS",
   "metadata": {
    "id": "kS0pN2EEOHFS"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpS1zR46OHFS",
   "metadata": {
    "id": "fpS1zR46OHFS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T8bYfqW0OHFS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8bYfqW0OHFS",
    "outputId": "0e457b24-c049-4984-ed6b-29f344f613b0"
   },
   "outputs": [],
   "source": [
    "# #Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hwsu0fkMOHFS",
   "metadata": {
    "id": "hwsu0fkMOHFS"
   },
   "outputs": [],
   "source": [
    "df['whole_txt'] = df['headline']+ ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGK5t8RyOHFT",
   "metadata": {
    "id": "tGK5t8RyOHFT"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove all non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "df['processed'] = df['whole_txt'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Tokenize before word2vec\n",
    "df['tokenized'] = df['processed'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(word for sentence in df['tokenized'] for word in sentence)\n",
    "total_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model with tokenized features\n",
    "from gensim.models import Word2Vec\n",
    "sentences = df['tokenized'].tolist()\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "df['vectorized'] = df['tokenized'].apply(lambda x: vectorize_text(x, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(df[\"relevance\"].apply(lambda x: 0 if x == \"no\" else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = np.array(df['vectorized'].tolist())\n",
    "y = df['relevance']  # Replace with your actual target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83826301",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fB_vM2GQkf-Y",
   "metadata": {
    "id": "fB_vM2GQkf-Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0850ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8059d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).astype('float32')\n",
    "X_val = np.array(X_val).astype('float32')\n",
    "y_train = np.array(y_train).astype('float32')\n",
    "y_val = np.array(y_val).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8e8b2",
   "metadata": {
    "id": "0fe8e8b2"
   },
   "source": [
    "### Building the model\n",
    "\n",
    "We are going to build a simple model that includes:\n",
    "- `Embedding` layer with an output representation of each word as a vector of dim 16\n",
    "- `LSTM` (see class slides for more detail or RNNs example notebook for more details) with an intermediate state of 100\n",
    "- An output layer `Dense` that connects the output of the LSTM and creates an output of 3 positions (one per class) as output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223fed7",
   "metadata": {},
   "source": [
    "That is model nr.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alirammal/Github/EconNewsMLIdent/RNN_Model_Basic 50-50_word2vec.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alirammal/Github/EconNewsMLIdent/RNN_Model_Basic%2050-50_word2vec.ipynb#Y232sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m unique_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(word \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mtokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alirammal/Github/EconNewsMLIdent/RNN_Model_Basic%2050-50_word2vec.ipynb#Y232sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m total_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(unique_words)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = word2vec_model.vector_size\n",
    "print(\"Word2Vec Embedding Dimension:\", embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300 \n",
    "\n",
    "def convert_to_sequence(text_tokens, model, max_length):\n",
    "    sequence = [model.wv[word] for word in text_tokens if word in model.wv]\n",
    "    # Pad with zeros if the sequence is shorter than max_length\n",
    "    padding = [np.zeros(model.vector_size)] * (max_length - len(sequence))\n",
    "    sequence.extend(padding)\n",
    "    # Truncate if the sequence is longer than max_length\n",
    "    return sequence[:max_length]\n",
    "\n",
    "X_train = np.array([convert_to_sequence(tokens, word2vec_model, max_length) for tokens in train_tokenized_texts])\n",
    "# Similarly for X_val and X_test\n",
    "X_val = np.array([convert_to_sequence(tokens, word2vec_model, max_length) for tokens in train_tokenized_texts])\n",
    "X_test = np.array([convert_to_sequence(tokens, word2vec_model, max_length) for tokens in train_tokenized_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
