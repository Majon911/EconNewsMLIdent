{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               8000 non-null   int64  \n",
      " 1   _golden                8000 non-null   bool   \n",
      " 2   _unit_state            8000 non-null   object \n",
      " 3   _trusted_judgments     8000 non-null   int64  \n",
      " 4   _last_judgment_at      8000 non-null   object \n",
      " 5   positivity             1420 non-null   float64\n",
      " 6   positivity:confidence  3775 non-null   float64\n",
      " 7   relevance              8000 non-null   object \n",
      " 8   relevance:confidence   8000 non-null   float64\n",
      " 9   articleid              8000 non-null   object \n",
      " 10  date                   8000 non-null   object \n",
      " 11  headline               8000 non-null   object \n",
      " 12  positivity_gold        0 non-null      float64\n",
      " 13  relevance_gold         0 non-null      float64\n",
      " 14  text                   8000 non-null   object \n",
      "dtypes: bool(1), float64(5), int64(2), object(7)\n",
      "memory usage: 882.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('US-Economic-News.csv', delimiter=',', encoding= 'ISO-8859-1')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842613455</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>8/14/91</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842613456</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 16:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>8/21/07</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842613457</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 1:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_398284048</td>\n",
       "      <td>11/14/91</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>842613458</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 2:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.675</td>\n",
       "      <td>wsj_397959018</td>\n",
       "      <td>6/16/86</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842613459</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398838054</td>\n",
       "      <td>10/4/02</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  842613455    False   finalized                   3     12/5/15 17:48   \n",
       "1  842613456    False   finalized                   3     12/5/15 16:54   \n",
       "2  842613457    False   finalized                   3      12/5/15 1:59   \n",
       "3  842613458    False   finalized                   3      12/5/15 2:19   \n",
       "4  842613459    False   finalized                   3     12/5/15 17:48   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         3.0                 0.6400       yes                 0.640   \n",
       "1         NaN                    NaN        no                 1.000   \n",
       "2         NaN                    NaN        no                 1.000   \n",
       "3         NaN                 0.0000        no                 0.675   \n",
       "4         3.0                 0.3257       yes                 0.640   \n",
       "\n",
       "       articleid      date                                           headline  \\\n",
       "0  wsj_398217788   8/14/91              Yields on CDs Fell in the Latest Week   \n",
       "1  wsj_399019502   8/21/07  The Morning Brief: White House Seeks to Limit ...   \n",
       "2  wsj_398284048  11/14/91  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3  wsj_397959018   6/16/86  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4  wsj_398838054   10/4/02  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "   positivity_gold  relevance_gold  \\\n",
       "0              NaN             NaN   \n",
       "1              NaN             NaN   \n",
       "2              NaN             NaN   \n",
       "3              NaN             NaN   \n",
       "4              NaN             NaN   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tengo que agarrar y cambiar los dtype de las variables con encoding\n",
    "# drop positivity gold and relevance gold\n",
    "# positivity and ::confidence, have very sparse values\n",
    "# _unit_state has only the finalized observation and golden the False observation, and trusted judgments only observation '3'\n",
    "\n",
    "df = df.drop(columns = ['_unit_id', 'positivity_gold', 'relevance_gold', '_unit_state', '_golden', '_trusted_judgments',\n",
    "                         '_unit_state', '_last_judgment_at', 'positivity', 'positivity:confidence'])\n",
    "\n",
    "df['articleid'] = df['articleid'].str.slice(0, 3)\n",
    "# so far, these variables SHOULD be dropped for sure as well as unit id and article id\n",
    "# should discuss between the positivity and positivity confidence due to amount of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "articleid\n",
       "wsj    938\n",
       "wap    482\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "filtered_df = df[df['relevance'] == 'yes']\n",
    "\n",
    "filtered_df['articleid'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['relevance'])\n",
    "y = df['relevance']\n",
    "\n",
    "# here stratify is used to balance dataset as much as possible\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance\n",
      "no          5257\n",
      "yes         1136\n",
      "not sure       7\n",
      "Name: count, dtype: int64\n",
      "relevance\n",
      "no          1314\n",
      "yes          284\n",
      "not sure       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "# is kind of balanced yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Ensure you have downloaded the necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "train_tokenized_text = [preprocess_text(doc) for doc in X_train['text']]\n",
    "train_tokenized_head = [preprocess_text(doc) for doc in X_train['headline']]\n",
    "\n",
    "\n",
    "X_train['clean_text'] = df['text'].apply(preprocess_text)\n",
    "X_train['clean_headline'] = df['headline'].apply(preprocess_text)\n",
    "X_test['clean_text'] = df['text'].apply(preprocess_text)\n",
    "X_test['clean_headline'] = df['headline'].apply(preprocess_text)\n",
    "\n",
    "combined_tokenized_text = train_tokenized_text + train_tokenized_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(combined_tokenized_text, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Vectorize each column in training and test sets\n",
    "X_train['vectorized_text'] = [document_vector(word2vec_model, doc) for doc in X_train['clean_text']]\n",
    "X_train['vectorized_headline'] = [document_vector(word2vec_model, doc) for doc in X_train['clean_headline']]\n",
    "\n",
    "X_test['vectorized_text'] = [document_vector(word2vec_model, doc) for doc in X_test['clean_text']]\n",
    "X_test['vectorized_headline'] = [document_vector(word2vec_model, doc) for doc in X_test['clean_headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['headline', 'text', 'clean_text', 'clean_headline'])\n",
    "X_test = X_test.drop(columns=['headline', 'text', 'clean_text', 'clean_headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>vectorized_text</th>\n",
       "      <th>vectorized_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wap</td>\n",
       "      <td>9/28/86</td>\n",
       "      <td>[0.087300286, 0.011467085, -0.03953612, 0.0503...</td>\n",
       "      <td>[0.14060357, 0.00290192, -0.037105225, 0.04222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wsj</td>\n",
       "      <td>3/28/96</td>\n",
       "      <td>[0.09097697, 0.0067355796, -0.032678287, 0.049...</td>\n",
       "      <td>[0.12639074, 0.026700534, -0.04324411, -0.0019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7059</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wap</td>\n",
       "      <td>1/9/93</td>\n",
       "      <td>[0.09460699, 0.003948897, -0.044204634, 0.0554...</td>\n",
       "      <td>[-0.015157646, -0.03469068, -0.018657232, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wap</td>\n",
       "      <td>2/9/00</td>\n",
       "      <td>[0.09538122, 0.010254582, -0.030811192, 0.0432...</td>\n",
       "      <td>[0.09177009, 0.045725744, -0.030763423, -0.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wap</td>\n",
       "      <td>8/23/75</td>\n",
       "      <td>[0.080340706, 0.00939782, -0.021030651, 0.0446...</td>\n",
       "      <td>[0.10776859, 0.00045859083, -0.026210537, 0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relevance:confidence articleid     date  \\\n",
       "6006                   1.0       wap  9/28/86   \n",
       "2304                   1.0       wsj  3/28/96   \n",
       "7059                   1.0       wap   1/9/93   \n",
       "5880                   1.0       wap   2/9/00   \n",
       "6700                   1.0       wap  8/23/75   \n",
       "\n",
       "                                        vectorized_text  \\\n",
       "6006  [0.087300286, 0.011467085, -0.03953612, 0.0503...   \n",
       "2304  [0.09097697, 0.0067355796, -0.032678287, 0.049...   \n",
       "7059  [0.09460699, 0.003948897, -0.044204634, 0.0554...   \n",
       "5880  [0.09538122, 0.010254582, -0.030811192, 0.0432...   \n",
       "6700  [0.080340706, 0.00939782, -0.021030651, 0.0446...   \n",
       "\n",
       "                                    vectorized_headline  \n",
       "6006  [0.14060357, 0.00290192, -0.037105225, 0.04222...  \n",
       "2304  [0.12639074, 0.026700534, -0.04324411, -0.0019...  \n",
       "7059  [-0.015157646, -0.03469068, -0.018657232, 0.07...  \n",
       "5880  [0.09177009, 0.045725744, -0.030763423, -0.085...  \n",
       "6700  [0.10776859, 0.00045859083, -0.026210537, 0.02...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac73f219ecb0e616815df33a5f0d272ef4060a3ce258f788ec56e0f80c4efd9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
